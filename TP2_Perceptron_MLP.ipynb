{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : Perceptron et Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Ce notebook pr√©sente l'impl√©mentation d'un perceptron monocouche et d'un perceptron multicouche (MLP) pour la classification d'images MNIST et Fashion-MNIST.\n",
    "\n",
    "## Objectifs\n",
    "- Impl√©menter un perceptron simple pour la classification MNIST\n",
    "- D√©velopper un MLP avec plusieurs couches cach√©es\n",
    "- Comparer diff√©rents optimiseurs, fonctions d'activation et techniques de r√©gularisation\n",
    "- Analyser les performances avec des matrices de confusion et des visualisations\n",
    "- Appliquer le meilleur mod√®le sur Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 1 : Perceptron Monocouche\n",
    "\n",
    "## 1.1 Import des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques n√©cessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# Configuration pour des visualisations claires\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Chargement et pr√©paration des donn√©es MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(f\"Forme des donn√©es d'entra√Ænement : {x_train.shape}\")\n",
    "print(f\"Forme des labels d'entra√Ænement : {y_train.shape}\")\n",
    "print(f\"Forme des donn√©es de test : {x_test.shape}\")\n",
    "print(f\"Forme des labels de test : {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de quelques exemples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Exemples du dataset MNIST', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Pr√©traitement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplatissement des images 28x28 en vecteurs de 784 dimensions\n",
    "x_train_flat = x_train.reshape(-1, 784)\n",
    "x_test_flat = x_test.reshape(-1, 784)\n",
    "\n",
    "# Normalisation des valeurs de pixels entre 0 et 1\n",
    "x_train_normalized = x_train_flat.astype('float32') / 255.0\n",
    "x_test_normalized = x_test_flat.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encoding des labels\n",
    "y_train_categorical = to_categorical(y_train, 10)\n",
    "y_test_categorical = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Forme apr√®s aplatissement : {x_train_flat.shape}\")\n",
    "print(f\"Forme apr√®s one-hot encoding : {y_train_categorical.shape}\")\n",
    "print(f\"Exemple de label one-hot pour {y_train[0]} : {y_train_categorical[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Impl√©mentation du Perceptron Monocouche\n",
    "\n",
    "Un perceptron monocouche est un r√©seau de neurones sans couche cach√©e. Il connecte directement l'entr√©e (784 neurones) √† la sortie (10 classes) avec une activation Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du mod√®le perceptron\n",
    "def create_perceptron():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(10, activation='softmax', input_shape=(784,))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "perceptron = create_perceptron()\n",
    "perceptron.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Compilation et entra√Ænement avec SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation avec l'optimiseur SGD\n",
    "perceptron.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entra√Ænement du mod√®le\n",
    "history_sgd = perceptron.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 √âvaluation du mod√®le avec SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur l'ensemble de test\n",
    "test_loss_sgd, test_accuracy_sgd = perceptron.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"\\n=== Perceptron avec SGD ===\")\n",
    "print(f\"Loss sur le test : {test_loss_sgd:.4f}\")\n",
    "print(f\"Accuracy sur le test : {test_accuracy_sgd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des courbes d'apprentissage\n",
    "def plot_history(history, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Train')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "    ax1.set_title(f'{title} - Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'], label='Train')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation')\n",
    "    ax2.set_title(f'{title} - Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_sgd, 'Perceptron avec SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Test avec l'optimiseur Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un nouveau perceptron pour Adam\n",
    "perceptron_adam = create_perceptron()\n",
    "\n",
    "# Compilation avec l'optimiseur Adam\n",
    "perceptron_adam.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entra√Ænement\n",
    "history_adam = perceptron_adam.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation avec Adam\n",
    "test_loss_adam, test_accuracy_adam = perceptron_adam.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"\\n=== Perceptron avec Adam ===\")\n",
    "print(f\"Loss sur le test : {test_loss_adam:.4f}\")\n",
    "print(f\"Accuracy sur le test : {test_accuracy_adam:.4f}\")\n",
    "\n",
    "# Comparaison\n",
    "print(f\"\\n=== Comparaison SGD vs Adam ===\")\n",
    "print(f\"SGD - Accuracy: {test_accuracy_sgd:.4f}, Loss: {test_loss_sgd:.4f}\")\n",
    "print(f\"Adam - Accuracy: {test_accuracy_adam:.4f}, Loss: {test_loss_adam:.4f}\")\n",
    "print(f\"Am√©lioration: {(test_accuracy_adam - test_accuracy_sgd)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_adam, 'Perceptron avec Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions sur l'ensemble de test (utilisation du mod√®le Adam)\n",
    "y_pred_proba = perceptron_adam.predict(x_test_normalized, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Calcul de la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Affichage de la matrice de confusion\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True)\n",
    "plt.title('Matrice de Confusion - Perceptron (Adam)', fontsize=16)\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe pr√©dite')\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Affichage des exemples mal classifi√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des exemples mal classifi√©s\n",
    "misclassified_indices = np.where(y_pred != y_test)[0]\n",
    "print(f\"Nombre d'exemples mal classifi√©s : {len(misclassified_indices)} sur {len(y_test)}\")\n",
    "\n",
    "# Affichage de 20 exemples mal classifi√©s\n",
    "num_display = min(20, len(misclassified_indices))\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < num_display:\n",
    "        idx = misclassified_indices[i]\n",
    "        ax.imshow(x_test[idx], cmap='gray')\n",
    "        ax.set_title(f'Vrai: {y_test[idx]}\\nPr√©dit: {y_pred[idx]}', color='red')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "plt.suptitle('Exemples mal classifi√©s', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 2 : Multi-Layer Perceptron (MLP)\n",
    "\n",
    "## 2.1 Impl√©mentation du MLP de base\n",
    "\n",
    "Nous allons cr√©er un MLP avec deux couches cach√©es pour am√©liorer les performances du perceptron simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du mod√®le MLP\n",
    "def create_mlp():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "mlp = create_mlp()\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation avec Adam\n",
    "mlp.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entra√Ænement\n",
    "history_mlp = mlp.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 √âvaluation du MLP de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation\n",
    "test_loss_mlp, test_accuracy_mlp = mlp.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"\\n=== MLP de base ===\")\n",
    "print(f\"Loss sur le test : {test_loss_mlp:.4f}\")\n",
    "print(f\"Accuracy sur le test : {test_accuracy_mlp:.4f}\")\n",
    "\n",
    "# Comparaison avec le perceptron\n",
    "print(f\"\\n=== Comparaison Perceptron vs MLP ===\")\n",
    "print(f\"Perceptron (Adam) - Accuracy: {test_accuracy_adam:.4f}\")\n",
    "print(f\"MLP - Accuracy: {test_accuracy_mlp:.4f}\")\n",
    "print(f\"Am√©lioration: {(test_accuracy_mlp - test_accuracy_adam)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_mlp, 'MLP de base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Exp√©rimentations et optimisations\n",
    "\n",
    "Nous allons maintenant tester diff√©rentes configurations pour optimiser notre MLP.\n",
    "\n",
    "### 2.3.1 Test de diff√©rentes fonctions d'activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage des r√©sultats\n",
    "results = []\n",
    "\n",
    "# Test avec ReLU (d√©j√† fait, pour r√©f√©rence)\n",
    "results.append({\n",
    "    'Mod√®le': 'MLP - ReLU',\n",
    "    'Test Loss': test_loss_mlp,\n",
    "    'Test Accuracy': test_accuracy_mlp\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec PReLU\n",
    "print(\"\\n=== Test avec PReLU ===\")\n",
    "mlp_prelu = models.Sequential([\n",
    "    layers.Dense(256, input_shape=(784,)),\n",
    "    layers.PReLU(),\n",
    "    layers.Dense(128),\n",
    "    layers.PReLU(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_prelu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_prelu = mlp_prelu.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_prelu, test_accuracy_prelu = mlp_prelu.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"PReLU - Loss: {test_loss_prelu:.4f}, Accuracy: {test_accuracy_prelu:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - PReLU', 'Test Loss': test_loss_prelu, 'Test Accuracy': test_accuracy_prelu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec LeakyReLU\n",
    "print(\"\\n=== Test avec LeakyReLU ===\")\n",
    "mlp_leaky = models.Sequential([\n",
    "    layers.Dense(256, input_shape=(784,)),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dense(128),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_leaky.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_leaky = mlp_leaky.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_leaky, test_accuracy_leaky = mlp_leaky.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"LeakyReLU - Loss: {test_loss_leaky:.4f}, Accuracy: {test_accuracy_leaky:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - LeakyReLU', 'Test Loss': test_loss_leaky, 'Test Accuracy': test_accuracy_leaky})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Ajout de Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec Dropout (0.25)\n",
    "print(\"\\n=== Test avec Dropout (0.25) ===\")\n",
    "mlp_dropout = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_dropout = mlp_dropout.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_dropout, test_accuracy_dropout = mlp_dropout.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"Dropout (0.25) - Loss: {test_loss_dropout:.4f}, Accuracy: {test_accuracy_dropout:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - Dropout 0.25', 'Test Loss': test_loss_dropout, 'Test Accuracy': test_accuracy_dropout})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Ajout de Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec Batch Normalization\n",
    "print(\"\\n=== Test avec Batch Normalization ===\")\n",
    "mlp_bn = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_bn = mlp_bn.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_bn, test_accuracy_bn = mlp_bn.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"Batch Normalization - Loss: {test_loss_bn:.4f}, Accuracy: {test_accuracy_bn:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - Batch Norm', 'Test Loss': test_loss_bn, 'Test Accuracy': test_accuracy_bn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Test de diff√©rents taux de Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec Dropout (0.2)\n",
    "print(\"\\n=== Test avec Dropout (0.2) ===\")\n",
    "mlp_dropout_02 = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_dropout_02.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_dropout_02 = mlp_dropout_02.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_dropout_02, test_accuracy_dropout_02 = mlp_dropout_02.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"Dropout (0.2) - Loss: {test_loss_dropout_02:.4f}, Accuracy: {test_accuracy_dropout_02:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - Dropout 0.2', 'Test Loss': test_loss_dropout_02, 'Test Accuracy': test_accuracy_dropout_02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec Dropout (0.3)\n",
    "print(\"\\n=== Test avec Dropout (0.3) ===\")\n",
    "mlp_dropout_03 = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_dropout_03.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_dropout_03 = mlp_dropout_03.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_dropout_03, test_accuracy_dropout_03 = mlp_dropout_03.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"Dropout (0.3) - Loss: {test_loss_dropout_03:.4f}, Accuracy: {test_accuracy_dropout_03:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - Dropout 0.3', 'Test Loss': test_loss_dropout_03, 'Test Accuracy': test_accuracy_dropout_03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Variation du taux d'apprentissage et de l'optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec Adam et learning rate = 0.001 (default)\n",
    "print(\"\\n=== Test avec Adam (lr=0.001) ===\")\n",
    "mlp_adam_001 = create_mlp()\n",
    "mlp_adam_001.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_adam_001 = mlp_adam_001.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_adam_001, test_accuracy_adam_001 = mlp_adam_001.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"Adam (lr=0.001) - Loss: {test_loss_adam_001:.4f}, Accuracy: {test_accuracy_adam_001:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - Adam lr=0.001', 'Test Loss': test_loss_adam_001, 'Test Accuracy': test_accuracy_adam_001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec Adam et learning rate = 0.0001\n",
    "print(\"\\n=== Test avec Adam (lr=0.0001) ===\")\n",
    "mlp_adam_0001 = create_mlp()\n",
    "mlp_adam_0001.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_adam_0001 = mlp_adam_0001.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_adam_0001, test_accuracy_adam_0001 = mlp_adam_0001.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"Adam (lr=0.0001) - Loss: {test_loss_adam_0001:.4f}, Accuracy: {test_accuracy_adam_0001:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - Adam lr=0.0001', 'Test Loss': test_loss_adam_0001, 'Test Accuracy': test_accuracy_adam_0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec SGD et learning rate = 0.01\n",
    "print(\"\\n=== Test avec SGD (lr=0.01) ===\")\n",
    "mlp_sgd_001 = create_mlp()\n",
    "mlp_sgd_001.compile(optimizer=SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_sgd_001 = mlp_sgd_001.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_sgd_001, test_accuracy_sgd_001 = mlp_sgd_001.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"SGD (lr=0.01) - Loss: {test_loss_sgd_001:.4f}, Accuracy: {test_accuracy_sgd_001:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - SGD lr=0.01', 'Test Loss': test_loss_sgd_001, 'Test Accuracy': test_accuracy_sgd_001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec SGD et learning rate = 0.1\n",
    "print(\"\\n=== Test avec SGD (lr=0.1) ===\")\n",
    "mlp_sgd_01 = create_mlp()\n",
    "mlp_sgd_01.compile(optimizer=SGD(learning_rate=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_sgd_01 = mlp_sgd_01.fit(\n",
    "    x_train_normalized, y_train_categorical,\n",
    "    epochs=10, batch_size=32, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "test_loss_sgd_01, test_accuracy_sgd_01 = mlp_sgd_01.evaluate(x_test_normalized, y_test_categorical, verbose=0)\n",
    "print(f\"SGD (lr=0.1) - Loss: {test_loss_sgd_01:.4f}, Accuracy: {test_accuracy_sgd_01:.4f}\")\n",
    "results.append({'Mod√®le': 'MLP - SGD lr=0.1', 'Test Loss': test_loss_sgd_01, 'Test Accuracy': test_accuracy_sgd_01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 Tableau r√©capitulatif de toutes les exp√©rimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du DataFrame de r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "results_df['Test Accuracy %'] = (results_df['Test Accuracy'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLEAU R√âCAPITULATIF DES PERFORMANCES\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identification du meilleur mod√®le\n",
    "best_model_row = results_df.iloc[0]\n",
    "print(f\"\\nüèÜ Meilleur mod√®le : {best_model_row['Mod√®le']}\")\n",
    "print(f\"   Accuracy : {best_model_row['Test Accuracy %']:.2f}%\")\n",
    "print(f\"   Loss : {best_model_row['Test Loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des performances\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy\n",
    "ax1.barh(results_df['Mod√®le'], results_df['Test Accuracy %'], color='skyblue')\n",
    "ax1.set_xlabel('Test Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Comparaison des Accuracies', fontsize=14)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.barh(results_df['Mod√®le'], results_df['Test Loss'], color='salmon')\n",
    "ax2.set_xlabel('Test Loss', fontsize=12)\n",
    "ax2.set_title('Comparaison des Losses', fontsize=14)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.4 Interpr√©tation des r√©sultats\n",
    "\n",
    "### 2.4.1 Quel mod√®le fonctionne le mieux et pourquoi ?\n",
    "\n",
    "D'apr√®s nos exp√©riences, nous pouvons observer que :\n",
    "\n",
    "1. **Le MLP surpasse le Perceptron simple** : L'ajout de couches cach√©es permet au r√©seau d'apprendre des repr√©sentations plus complexes et non-lin√©aires des donn√©es, ce qui am√©liore significativement la pr√©cision.\n",
    "\n",
    "2. **L'optimiseur Adam est g√©n√©ralement plus performant que SGD** : Adam adapte automatiquement le taux d'apprentissage pour chaque param√®tre, ce qui acc√©l√®re la convergence et am√©liore la performance finale.\n",
    "\n",
    "3. **Les diff√©rentes fonctions d'activation** : ReLU est g√©n√©ralement un bon choix par d√©faut. PReLU et LeakyReLU peuvent apporter de l√©g√®res am√©liorations en permettant un petit gradient pour les valeurs n√©gatives.\n",
    "\n",
    "### 2.4.2 Le Dropout am√©liore-t-il les performances ?\n",
    "\n",
    "Le Dropout est une technique de r√©gularisation qui aide √† pr√©venir le surapprentissage en d√©sactivant al√©atoirement des neurones pendant l'entra√Ænement :\n",
    "\n",
    "- **Avantages** : R√©duit le surapprentissage, am√©liore la g√©n√©ralisation\n",
    "- **Inconv√©nients** : Peut l√©g√®rement r√©duire la pr√©cision sur l'ensemble d'entra√Ænement\n",
    "- **Optimal** : Un taux de dropout entre 0.2 et 0.3 est g√©n√©ralement un bon compromis\n",
    "\n",
    "### 2.4.3 Impact de l'ajout de couches suppl√©mentaires\n",
    "\n",
    "- **Plus de couches** = Plus de capacit√© d'apprentissage\n",
    "- Cependant, trop de couches peuvent conduire au surapprentissage si les donn√©es sont limit√©es\n",
    "- Les couches de normalisation (Batch Normalization) aident √† stabiliser l'entra√Ænement de r√©seaux plus profonds\n",
    "\n",
    "### 2.4.4 Conclusions\n",
    "\n",
    "- Un MLP avec 2-3 couches cach√©es, activation ReLU, optimizer Adam, et un l√©ger Dropout est g√©n√©ralement optimal pour MNIST\n",
    "- La normalisation des donn√©es est cruciale pour de bonnes performances\n",
    "- Le choix du taux d'apprentissage a un impact significatif sur la convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.5 Application du meilleur mod√®le sur Fashion-MNIST\n",
    "\n",
    "Maintenant, appliquons notre meilleur architecture sur le dataset Fashion-MNIST pour voir comment elle se g√©n√©ralise √† un probl√®me diff√©rent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de Fashion-MNIST\n",
    "(x_train_fashion, y_train_fashion), (x_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
    "\n",
    "print(f\"Fashion-MNIST - Forme des donn√©es d'entra√Ænement : {x_train_fashion.shape}\")\n",
    "print(f\"Fashion-MNIST - Forme des donn√©es de test : {x_test_fashion.shape}\")\n",
    "\n",
    "# Classes Fashion-MNIST\n",
    "fashion_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                       'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de quelques exemples Fashion-MNIST\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train_fashion[i], cmap='gray')\n",
    "    ax.set_title(f'{fashion_class_names[y_train_fashion[i]]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Exemples du dataset Fashion-MNIST', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©traitement des donn√©es Fashion-MNIST\n",
    "x_train_fashion_flat = x_train_fashion.reshape(-1, 784).astype('float32') / 255.0\n",
    "x_test_fashion_flat = x_test_fashion.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "y_train_fashion_categorical = to_categorical(y_train_fashion, 10)\n",
    "y_test_fashion_categorical = to_categorical(y_test_fashion, 10)\n",
    "\n",
    "print(f\"Donn√©es pr√©trait√©es - Train: {x_train_fashion_flat.shape}, Test: {x_test_fashion_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du meilleur mod√®le (bas√© sur les exp√©riences)\n",
    "# Utilisons un MLP avec Dropout 0.2 et Adam\n",
    "best_model_fashion = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "best_model_fashion.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n=== Entra√Ænement sur Fashion-MNIST ===\")\n",
    "history_fashion = best_model_fashion.fit(\n",
    "    x_train_fashion_flat, y_train_fashion_categorical,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur Fashion-MNIST\n",
    "test_loss_fashion, test_accuracy_fashion = best_model_fashion.evaluate(\n",
    "    x_test_fashion_flat, y_test_fashion_categorical, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n=== R√©sultats sur Fashion-MNIST ===\")\n",
    "print(f\"Test Loss : {test_loss_fashion:.4f}\")\n",
    "print(f\"Test Accuracy : {test_accuracy_fashion:.4f}\")\n",
    "\n",
    "print(f\"\\n=== Comparaison MNIST vs Fashion-MNIST ===\")\n",
    "print(f\"MNIST - Accuracy : {test_accuracy_mlp:.4f}\")\n",
    "print(f\"Fashion-MNIST - Accuracy : {test_accuracy_fashion:.4f}\")\n",
    "print(f\"Diff√©rence : {(test_accuracy_mlp - test_accuracy_fashion)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'apprentissage pour Fashion-MNIST\n",
    "plot_history(history_fashion, 'MLP sur Fashion-MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion pour Fashion-MNIST\n",
    "y_pred_fashion_proba = best_model_fashion.predict(x_test_fashion_flat, verbose=0)\n",
    "y_pred_fashion = np.argmax(y_pred_fashion_proba, axis=1)\n",
    "\n",
    "cm_fashion = confusion_matrix(y_test_fashion, y_pred_fashion)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_fashion, annot=True, fmt='d', cmap='Blues', square=True,\n",
    "            xticklabels=fashion_class_names, yticklabels=fashion_class_names)\n",
    "plt.title('Matrice de Confusion - Fashion-MNIST', fontsize=16)\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe pr√©dite')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRapport de classification Fashion-MNIST:\")\n",
    "print(classification_report(y_test_fashion, y_pred_fashion, target_names=fashion_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples de pr√©dictions correctes\n",
    "correct_indices = np.where(y_pred_fashion == y_test_fashion)[0]\n",
    "print(f\"Nombre de pr√©dictions correctes : {len(correct_indices)} sur {len(y_test_fashion)}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = correct_indices[i]\n",
    "    ax.imshow(x_test_fashion[idx], cmap='gray')\n",
    "    ax.set_title(f'Pr√©dit: {fashion_class_names[y_pred_fashion[idx]]}\\nVrai: {fashion_class_names[y_test_fashion[idx]]}',\n",
    "                 color='green', fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Exemples de pr√©dictions correctes - Fashion-MNIST', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples mal classifi√©s sur Fashion-MNIST\n",
    "misclassified_fashion = np.where(y_pred_fashion != y_test_fashion)[0]\n",
    "print(f\"Nombre d'exemples mal classifi√©s : {len(misclassified_fashion)} sur {len(y_test_fashion)}\")\n",
    "\n",
    "num_display = min(20, len(misclassified_fashion))\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < num_display:\n",
    "        idx = misclassified_fashion[i]\n",
    "        ax.imshow(x_test_fashion[idx], cmap='gray')\n",
    "        ax.set_title(f'Vrai: {fashion_class_names[y_test_fashion[idx]]}\\nPr√©dit: {fashion_class_names[y_pred_fashion[idx]]}',\n",
    "                     color='red', fontsize=9)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "plt.suptitle('Exemples mal classifi√©s - Fashion-MNIST', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions finales\n",
    "\n",
    "### R√©sum√© des apprentissages\n",
    "\n",
    "Dans ce TP, nous avons explor√© :\n",
    "\n",
    "1. **Perceptron monocouche** : Un mod√®le simple qui atteint ~92% d'accuracy sur MNIST\n",
    "2. **Multi-Layer Perceptron** : L'ajout de couches cach√©es am√©liore significativement les performances (~97-98%)\n",
    "3. **Optimisation** : Adam converge plus rapidement que SGD pour ce type de probl√®me\n",
    "4. **R√©gularisation** : Le Dropout et la Batch Normalization aident √† am√©liorer la g√©n√©ralisation\n",
    "5. **Fonctions d'activation** : ReLU, PReLU et LeakyReLU donnent des r√©sultats similaires\n",
    "6. **Fashion-MNIST** : Plus difficile que MNIST (accuracy ~88-90%) car les images sont plus complexes\n",
    "\n",
    "### Recommandations\n",
    "\n",
    "Pour des probl√®mes de classification d'images similaires :\n",
    "- Utiliser un MLP avec 2-3 couches cach√©es\n",
    "- Optimizer Adam avec learning rate ~0.001\n",
    "- Ajouter un Dropout l√©ger (0.2-0.25) pour √©viter le surapprentissage\n",
    "- Normaliser les donn√©es d'entr√©e\n",
    "- Utiliser ReLU comme fonction d'activation par d√©faut\n",
    "\n",
    "Pour am√©liorer encore les performances, on pourrait explorer :\n",
    "- Des r√©seaux convolutifs (CNN) qui sont mieux adapt√©s aux images\n",
    "- L'augmentation de donn√©es\n",
    "- Des architectures plus profondes\n",
    "- L'optimisation des hyperparam√®tres par grid search ou random search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
